var documenterSearchIndex = {"docs":
[{"location":"polarization/#Polarization-Priors","page":"Polarization Priors","title":"Polarization Priors","text":"","category":"section"},{"location":"mrf/#Markov-Random-Field-Priors","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"","category":"section"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"VLBI imaging is an ill-posed problem. Namely there is an infinite number of images that reproduce the observations, especially since the true image often has many more degrees of freedom than what we actually observe. To counteract this, we require the additional information in the form of a prior. For VLBI imaging we are limited to the resolution of the telescope which is set by the distance from the two furthest sites. Any additional information we gain about the image structure is then given by some additional assumption of belief we have about the source.","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"A common spatial modeling prior used in other fields such as geostatistics are Markov Random Fields. These consist of priors that encode correlations between neighbors using a Markovian structure. This Markovian dependency induces sparsity in the precision matrix of Gaussian distribution. This spartsity is key to making Markov Random fields scale to large amounts of data and is the reason things like Kalman filters scale linearly in the number of data points while standard Gaussian processes scale cubically. ","category":"page"},{"location":"mrf/#","page":"Markov Random Field Priors","title":"","text":"","category":"section"},{"location":"mrf/#Relation-to-RML-Regularizers","page":"Markov Random Field Priors","title":"Relation to RML Regularizers","text":"","category":"section"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"In regularized maximum likelihood (RML) imaging these additional assumptions are encoded into regularizers that enforce things like smoothness, sparsity, similarity to some fiducial image structure. The cost function for regularized imaging is","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"    J_λ(I) = sum_d χ_d²(I) + sum_r alpha_r R(I) ","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"where R are the regularizers and alpha_r are the regularizer hyperparameters. The problem with RML imaging is that the values of the hyperparameters is often unknown. Therefore, to find their values people often use heuristics and surveys over different values to find what looks good. However, this can often lead to biases and imaging artifacts. ","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"However, probabilistic or statistical imaging provides a formalism simultaneously solve for the image and the regularizer or prior hyperparameters. For this we will assume that our prior p with hyperparameters alpha are of the form","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"    p(x  alpha) = N(alpha) f(x^T Q x)","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"whereQ is the scale matrix that sets the variation and correlation scale of the problem, f is a function, and N is the normalization. We then seek a set of distributions for which the inner-product x^t Q x and N are easy to compute. To accomplish this we will look at Markov Random Fields and specifically the PDE","category":"page"},{"location":"mrf/","page":"Markov Random Field Priors","title":"Markov Random Field Priors","text":"    (kappa^2 - Delta)^n varphi = mathcalW(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = VLBIImagePriors","category":"page"},{"location":"#VLBIImagePriors","page":"Home","title":"VLBIImagePriors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements a number of priors that are helpful when imaging VLBI data. These priors include commonly used Bayesian Stokes I imaging priors such as ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Log Uniform prior from Broderick et al. 2020\nDirichlet prior from Pesce 2021\nGaussian Markov Random Field (see Markov Random Field Priors)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For polarized imaging we also include a number of useful priors that parameterize the unit n-sphere which are required to parameterize the Poincaré sphere Polarization Priors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"As of v0.8 NamedDist has been moved to HypercubeTransforms.jl. If you depended on it please load that package instead.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Documentation for VLBIImagePriors.","category":"page"},{"location":"","page":"Home","title":"Home","text":"warn: Warn\nAs of 0.9 VLBIImagePriors requires you to load Enzyme explicitly even if using Zygote since  rules will generically call into Enzyme as needed.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [VLBIImagePriors]","category":"page"},{"location":"#VLBIImagePriors.AdditiveLR","page":"Home","title":"VLBIImagePriors.AdditiveLR","text":"AdditiveLR <: LogRatioTransform\n\nDefines the additive log-ratio transform. The clr transformation moves from the simplex Sⁿ → R^n-1 and is given by\n\nalr(x) = [log(x₁/xₙ) ... log(xₙ/xₙ)],\n\nwhere g(x) = (∏xᵢ)ⁿ⁻¹ is the geometric mean. The inverse transformation is given by\n\nalr⁻¹(x) = exp.(x)./(1 + sum(x[1:n-1])).\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.AngleTransform","page":"Home","title":"VLBIImagePriors.AngleTransform","text":"AngleTransform\n\nA transformation that moves two vector x and y to an angle θ.  Note that is x and y are normally distributed then the resulting distribution in θ is uniform on the circle.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.CenterImage-Tuple{ComradeBase.IntensityMap}","page":"Home","title":"VLBIImagePriors.CenterImage","text":"CenterImage(X, Y)\nCenterImage(img::IntensityMap)\nCenterImage(grid::RectiGrid)\n\nConstructs a projections operator that will take an arbritrary image and return a transformed version whose centroid is at the origin.\n\nArguments\n\nX, Y: the iterators for the image pixel locations\nimg: A IntensityMap whose grid is the same as the image you want to center\ngrid: The grid that the image is defined on.\n\nExample\n\njulia> using ComradeBase: centroid, imagepixels\njulia> grid = imagepixels(μas2rad(100.0), μas2rad(100.0), 256, 256)\njulia> K = CenterImage(grid)\njulia> img = IntensityMap(rand(256, 256), grid)\njulia> centroid(img)\n(1.34534e-10, 5.23423e-11)\njulia> cimg = K(img)\njulia> centroid(cimg)\n(1.231e-14, -2.43e-14)\n\nNote\n\nThis center image works using a linear projection operator. This means that is does not necessarily preserve the image total flux and the positive definiteness of the image. In practise we find that the deviation from the original flux, and the amount of negative flux is small.\n\nExplanation\n\nThe centroid is constructed by\n\nX ⋅ I = 0\nY ⋅ I = 0\n\nwhere I is the flattened image of length N, and X and Y are the image pixel locations. This can be simplified into the matrix equation\n\nXY ⋅ I = 0\n\nwhere XY is the 2×N matrix whose first for is given by X and second is Y. The space of centered images is then given by the null space of XY. Given this we can form a matrix C which is the kernel matrix of the nullspace whose columns are the orthogonal basis vectors of the null space of XY. Using this we can construct a centered image projection opertor by\n\nK = CC'.\n\nOur centered image is then given by I₀ = KI.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.CenteredLR","page":"Home","title":"VLBIImagePriors.CenteredLR","text":"CenteredLR <: LogRatioTransform\n\nDefines the centered log-ratio transform. The clr transformation moves from the simplex Sⁿ → Rⁿ and is given by\n\nclr(x) = [log(x₁/g(x)) ... log(xₙ/g(x))]\n\nwhere g(x) = (∏xᵢ)ⁿ⁻¹ is the geometric mean. The inverse transformation is given by the softmax function and is only defined on a subset of the domain otherwise it is not injective\n\nclr⁻¹(x) = exp.(x)./sum(exp, x).\n\nNotes\n\nAs mentioned above this transformation is bijective on the entire codomain of the function. However, unlike the additive log-ratio transform it does not treat any pixel as being special.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.CenteredRegularizer","page":"Home","title":"VLBIImagePriors.CenteredRegularizer","text":"CenteredRegularizer(x, y, σ, p)\n\nRegularizes a general image prior p such that the center of light is close the the origin of the imag. After regularization the log density of the prior is modified to\n\n    log p(I) to log p(I) - frac(x_C^2 + y_C^2)^22sigma^2 N_x N_y\n\nwhere N_x and N_y are the number of pixels in the x and y direction of the image, and x_C y_C are the center of light of the image I.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ConditionalMarkov-Tuple{Type{<:VLBIImagePriors.MarkovRandomField}, Vararg{Any}}","page":"Home","title":"VLBIImagePriors.ConditionalMarkov","text":"ConditionalMarkov(D, args...)\n\nCreates a Conditional Markov measure, that behaves as a Julia functional. The functional returns a probability measure defined by the arguments passed to the functional.\n\nArguments\n\nD: The <: MarkovRandomField that defines the underlying measure\nargs: Additional arguments used to construct the Markov random field cache.         See MarkovRandomFieldGraph for more information.\n\nExample\n\njulia> grid = imagepixels(10.0, 10.0, 64, 64)\njulia> ℓ = ConditionalMarkov(GaussMarkovRandomField, grid)\njulia> d = ℓ(16) # This is now a distribution\njulia> rand(d)\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.DiagonalVonMises","page":"Home","title":"VLBIImagePriors.DiagonalVonMises","text":"DiagonalVonMises(μ::Real, κ::Real)\nDiagonalVonMises(μ::AbstractVector{<:Real}, κ::AbstractVector{<:Real})\n\nConstructs a Von Mises distribution, with mean μ and concentraion parameter κ. If μ and κ are vectors then this constructs a independent multivariate Von Mises distribution.\n\nNotes\n\nThis is a custom implementation since the version in Distributions.jl has certain properties that do not play well (having an support only between [-π+μ, π+μ]) with usual VLBI problems. Additionally this distribution has a special overloaded product_distribution method so that concatenating multiple DiagonalVonMises together preserves the type.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.EMRF","page":"Home","title":"VLBIImagePriors.EMRF","text":"Alias for `ExpMarkovRandomField`\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ExpMarkovRandomField","page":"Home","title":"VLBIImagePriors.ExpMarkovRandomField","text":"struct ExpMarkovRandomField{T<:Number, C} <: VLBIImagePriors.MarkovRandomField\n\nA image prior based off of the zero mean unit variance Exponential Markov random field. The order of the Markov random field is specified\n\nFields\n\nρ: The correlation length of the random field.\n\ngraph: The Markov Random Field graph cache used to define the specific Markov random field class used.\n\nExample\n\njulia> ρ = 10.0\njulia> d = ExpMarkovRandomField(ρ, (32, 32))\njulia> cache = MarkovRandomFieldGraph(Float64, (32, 32)) # now instead construct the cache\njulia> d2 = ExpMarkovRandomField(ρ, cache)\njulia> scalematrix(d) ≈ scalematrix(d2)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ExpMarkovRandomField-Tuple{Number, AbstractMatrix}","page":"Home","title":"VLBIImagePriors.ExpMarkovRandomField","text":"ExpMarkovRandomField(ρ, img::AbstractArray; order::Integer=1)\n\nConstructs a orderᵗʰ order  Exponential Markov random field with dimensions size(img), correlation ρ and unit covariance.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.ExpMarkovRandomField-Tuple{Number, MarkovRandomFieldGraph}","page":"Home","title":"VLBIImagePriors.ExpMarkovRandomField","text":"ExpMarkovRandomField(ρ, cache::MarkovRandomFieldGraph)\n\nConstructs a first order zero-mean and unit variance Exponential Markov random field using the precomputed cache cache.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.ExpMarkovRandomField-Tuple{Number, Tuple{Int64, Int64}}","page":"Home","title":"VLBIImagePriors.ExpMarkovRandomField","text":"ExpMarkovRandomField(ρ, dims; order=1)\n\nConstructs a first order zero-mean unit variance Exponential Markov random field with dimensions dims, correlation ρ.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.GMRF","page":"Home","title":"VLBIImagePriors.GMRF","text":"Alias for `GaussMarkovRandomField`\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.GaussMarkovRandomField-Tuple{Number, AbstractMatrix}","page":"Home","title":"VLBIImagePriors.GaussMarkovRandomField","text":"GaussMarkovRandomField(ρ, img::AbstractArray; order::Integer=1)\n\nConstructs a orderᵗʰ order  Gaussian Markov random field with dimensions size(img), correlation ρ and unit covariance.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. Noting that order=1 is equivalent to the usual TSV and L₂ regularization from RML imaging. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.GaussMarkovRandomField-Tuple{Number, Tuple{Int64, Int64}}","page":"Home","title":"VLBIImagePriors.GaussMarkovRandomField","text":"GaussMarkovRandomField(ρ, dims; order=1)\nGaussMarkovRandomField(ρ, g::AbstractRectiGrid; order=1)\n\nConstructs a orderᵗʰ order Gaussian Markov random field with dimensions size(img), correlation ρ and unit covariance.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. Noting that order=1 is equivalent to the usual TSV and L₂ regularization from RML imaging. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.ImageDirichlet","page":"Home","title":"VLBIImagePriors.ImageDirichlet","text":"ImageDirichlet(α::AbstractMatrix)\nImageDirichlet(α::Real, ny, nx)\n\nA Dirichlet distribution defined on a matrix. Samples from this produce matrices whose elements sum to unity. This is a useful image prior when you want to separately constrain the flux. The  α parameter defines the usual Dirichlet concentration amount.\n\nNotes\n\nMuch of this code was taken from Distributions.jl and it's Dirichlet distribution. However, some changes were made to make it faster. Additionally, we use define a custom rrule to speed up derivatives.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ImageSimplex","page":"Home","title":"VLBIImagePriors.ImageSimplex","text":"ImageSimplex(ny,nx)\n\nThis defines a transformation from ℝⁿ⁻¹ to the n probability simplex defined on an matrix with dimension ny×nx. This is a more natural transformation for rasterized images, which are most naturally represented as a matrix.\n\nNotes\n\nMuch of this code was inspired by TransformVariables. However, we have specified custom rrules using Enzyme as a backend. This allowed the simplex transform to be used with Zygote and we achieved an order of magnitude speedup when computing the pullback of the simplex transform.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ImageSphericalUniform","page":"Home","title":"VLBIImagePriors.ImageSphericalUniform","text":"ImageSphericalUniform(nx, ny)\n\nConstruct a distribution where each image pixel is a 3-sphere uniform variable. This is useful for polarization where the stokes parameters are parameterized on the 3-sphere.\n\nCurrently we use a struct of vectors memory layout. That is the image is described by three matrices (X,Y,Z) grouped together as a tuple, where each matrix is one direction on the sphere, and we require norm((X,Y,Z)) == 1.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.ImageUniform","page":"Home","title":"VLBIImagePriors.ImageUniform","text":"ImageUniform(a::Real, b::Real, nx, ny)\n\nA uniform distribution in image pixels where a/b are the lower/upper bound for the interval. This then concatenates ny×nx uniform distributions together.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.MarkovRandomField","page":"Home","title":"VLBIImagePriors.MarkovRandomField","text":"abstract type MarkovRandomField <: Distributions.Distribution{Distributions.Matrixvariate, Distributions.Continuous}\n\nAn abstract type for a MarkovRandomField. We assume that the distribution is of the form\n\np(x | ρ) = N(detQ(ρ)) f(xᵀQ(ρ)x),\n\nwhere f is a function and N is the normalization of the distribution, and ρ is the correlation parameter.\n\nTo implement the informal interface e.g., MyRF <: MarkovRandomField, the user must implement\n\nlognorm(d::MyRF): Which computes the log of the normalization constant N\nunnormed_logpdf(d::MyRF, x::AbstractMatrix): Which computes f(xᵀQx)\nDistributions._rand!(rng::AbstractRNG, d::MyRF, x::AbstractMatrix): To enable sampling  from the prior\n\nAdditionally, there are a number of auto-generated function that can be overwritten:\n\ngraph(d::MyRF): Which returns the graph structure of the Markov Random Field.  The default returns the property d.graph.\ncorrparam(d::MyRF): Which returns the correlation parameter ρ of the Markov Random Field.  The default returns the property d.ρ.\nBase.size(d::MyRF): Which returns the size of the distribution. This defaults to the  size of the graph cache.\nscalematrix(d::MyRF): Which computes the scale matrix Q, of the random field. The  default is to forward to the scalematrix(graph(d), corrparm(d)).\n(c::ConditionalMarkov{<:MyRF})(ρ): To map from a correlation to the distribution\nHypercubeTransform.asflat(d::MyRF): To map from parameter space to a flattened version.  The default is TransformVariables.as(Matrix, size(d)...)\nDistributions.insupport(d::MyRF, x::AbstractMatrix) which checks if x is in the  support of d. The default is to always return true.\nLinearAlgebra.logdet(d::MyRF) which computes the log determinant of Q. This defaults to  logdet(graph(d), corrparam(d)).\n\nFinally additional optional methods are:\n\nDistributions.mean(d::MyRF): Which computes the mean of the process\nDistributions.cov(d::MyRF): Which computes the covariance matrix of the process.\nDistributions.invcov(d::MyRF): Computes the precision matrix of the random field\n\nFor an example implementation see e.g., GaussMarkovRandomField\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.MarkovRandomFieldGraph-Tuple{ComradeBase.AbstractRectiGrid}","page":"Home","title":"VLBIImagePriors.MarkovRandomFieldGraph","text":"MarkovRandomFieldGraph(grid::AbstractRectiGrid; order=1)\nMarkovRandomFieldGraph(img::AbstractMatrix; order=1)\n\nCreate a order Markov random field using the grid or image as its dimension.\n\nThe order keyword argument specifies the order of the Markov random process and is generally given by the precision matrix\n\nQₙ = τ(κI + G)ⁿ\n\nwhere n = order, I is the identity matrix, G is specified by the first order stencil\n\n.  -1  .\n-1  4  -1\n.   4  .\n\nκ is the Markov process hyper-parameters. For n=1 κ is related to the correlation length ρ of the random field by\n\nρ = 1/κ\n\nwhile for n>1 it is given by\n\nρ = √(8(n-1))/κ\n\nNote that κ isn't set in the MarkovRandomFieldGraph, but rather once the noise process is set, i.e. one of the subtypes of MarkovRandomField.\n\nFinally τ is chosen so that the marginal variance of the random field is unity. For n=1\n\nτ = 1\n\nfor n=2\n\nτ = 4πκ²\n\nand for n>2 we have\n\nτ = (N+1)4π κ²⁽ⁿ⁺¹⁾\n\nExample\n\njulia> m = MarkovRandomFieldGraph(imagepixels(10.0, 10.0, 64, 64))\njulia> ρ = 10 # correlation length\njulia> d = GaussMarkovRandomField(ρ, m) # build the Gaussian Markov random field\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.SphericalUnitVector","page":"Home","title":"VLBIImagePriors.SphericalUnitVector","text":"SphericalUnitVector{N}()\n\nA transformation from a set of N+1 vectors to the N sphere. The set of N+1 vectors are inherently assumed to be N+1 a distributed according to a unit multivariate Normal distribution.\n\nNotes\n\nFor more information about this transformation see the Stan manual. In the future this may be depricated when  is merged.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.TDistMarkovRandomField","page":"Home","title":"VLBIImagePriors.TDistMarkovRandomField","text":"struct TDistMarkovRandomField{T<:Number, C} <: VLBIImagePriors.MarkovRandomField\n\nA image prior based off of the first-order Multivariate T distribution Markov random field.\n\nFields\n\nρ: The correlation length of the random field.\n\nν: The student T \"degrees of freedom parameter which ≥ 1 for a proper prior\n\ngraph: The Markov Random Field graph cache used to define the specific Markov random field class used.\n\nExamples\n\njulia> ρ, ν = 16.0, 1.0\njulia> d = TDistMarkovRandomField(ρ, ν, (32, 32))\njulia> cache = MarkovRandomFieldGraph(Float64, (32, 32)) # now instead construct the cache\njulia> d2 = TDistMarkovRandomField(ρ, ν, cache)\njulia> invcov(d) ≈ invcov(d2)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.TDistMarkovRandomField-Tuple{Number, Number, AbstractMatrix}","page":"Home","title":"VLBIImagePriors.TDistMarkovRandomField","text":"TDistMarkovRandomField(ρ, ν, img::AbstractArray; order=1)\n\nConstructs a first order TDist Markov random field with zero median dimensions size(img), correlation ρ and degrees of freedom ν.\n\nNote ν ≥ 1 to be a well-defined probability distribution.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.TDistMarkovRandomField-Tuple{Number, Number, MarkovRandomFieldGraph}","page":"Home","title":"VLBIImagePriors.TDistMarkovRandomField","text":"TDistMarkovRandomField(ρ, ν, cache::MarkovRandomFieldGraph)\n\nConstructs a first order TDist Markov random field with zero mean ,correlation ρ, degrees of freedom ν, and the precomputed MarkovRandomFieldGraph cache.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.TDistMarkovRandomField-Tuple{Number, Number, Tuple{Int64, Int64}}","page":"Home","title":"VLBIImagePriors.TDistMarkovRandomField","text":"TDistMarkovRandomField(ρ, ν, dims)\n\nConstructs a first order TDist Markov random field with zero mean ,correlation ρ, degrees of freedom ν, with dimension dims.\n\nThe order parameter controls the smoothness of the field with higher orders being smoother. We recommend sticking with either order=1,2. For more information about the impact of the order see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.TMRF","page":"Home","title":"VLBIImagePriors.TMRF","text":"Alias for `TDistMarkovRandomField`\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.WrappedUniform","page":"Home","title":"VLBIImagePriors.WrappedUniform","text":"WrappedUniform(period)\n\nConstructs a potentially multivariate uniform distribution that is wrapped a given period. That is\n\nd = WrappedUniform(period)\nlogpdf(d, x) ≈ logpdf(d, x+period)\n\nfor any x.\n\nIf period is a vector this creates a multivariate independent wrapped uniform distribution.\n\n\n\n\n\n","category":"type"},{"location":"#VLBIImagePriors.alr!-Tuple{Any, Any}","page":"Home","title":"VLBIImagePriors.alr!","text":"alr!(x, y)\n\nCompute the inverse alr transform. That is x lives in ℜⁿ and y, lives in Δⁿ\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.alrinv!-Tuple{Any, Any}","page":"Home","title":"VLBIImagePriors.alrinv!","text":"alrinv!(x, y)\n\nComputes the additive logit transform inplace. This converts from ℜⁿ → Δⁿ where Δⁿ is the n-simplex\n\nNotes\n\nThis function is mainly to transform the GaussMarkovRandomField to live on the simplex. In order to preserve the nice properties of the GRMF namely the log det we only use y[begin:end-1] elements and the last one is not included in the transform. This shouldn't be a problem since the additional parameter is just a dummy in that case and the Gaussian prior should ensure it is easy to sample from.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.clr!-Tuple{Any, Any}","page":"Home","title":"VLBIImagePriors.clr!","text":"clr!(x, y)\n\nCompute the inverse alr transform. That is x lives in ℜⁿ and y, lives in Δⁿ\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.clrinv!-Tuple{Any, Any}","page":"Home","title":"VLBIImagePriors.clrinv!","text":"clrinv!(x, y)\n\nComputes the additive logit transform inplace. This converts from ℜⁿ → Δⁿ where Δⁿ is the n-simplex\n\nNotes\n\nThis function is mainly to transform the GaussMarkovRandomField to live on the simplex.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.corrparam-Tuple{VLBIImagePriors.MarkovRandomField}","page":"Home","title":"VLBIImagePriors.corrparam","text":"corrparam(m::MarkovRandomField)\n\nReturns the correlation parameter of the Markov Random field m. For details about the correlation parmeter see MarkovRandomFieldGraph.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.graph-Tuple{VLBIImagePriors.MarkovRandomField}","page":"Home","title":"VLBIImagePriors.graph","text":"graph(m::MarkovRandomField)\n\nReturns the graph or graph cache of the Markov Random field m.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.matern-Tuple{Type{<:Number}, Tuple{Int64, Int64}}","page":"Home","title":"VLBIImagePriors.matern","text":"matern([T=Float64], dims::Dims{2})\nmatern([T=Float64], dims::Int...)\n\nCreates an approximate Matern Gaussian process that approximates the Matern process on a regular grid which cyclic boundary conditions. This function returns a tuple of two objects\n\nA functor f of type StationaryMatern that iid-Normal noise to a draw from the Matern process. The functor call arguments are f(s, ρ, ν) where s is the random white Gaussian noise with dimension dims, ρ is the correlation length, and ν is Matern smoothness parameter\nThe a set of prod(dims) standard Normal distributions that can serve as the noise generator for the process.\n\nExample\n\njulia> transform, dstd = matern((32, 32))\njulia> draw_matern = transform(rand(dstd), 10.0, 2.0)\njulia> draw_matern_aniso = transform(rand(dstd), (10.0, 5.0), π/4 2.0) # anisotropic Matern\njulia> ones(32, 32) .+ 5.* draw_matern # change the mean and variance of the field\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.matern-Union{Tuple{AbstractMatrix{T}}, Tuple{T}} where T","page":"Home","title":"VLBIImagePriors.matern","text":"matern(img::AbstractMatrix)\n\nCreates an approximate Matern Gaussian process with dimension size(img)\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.scalematrix-Tuple{VLBIImagePriors.MarkovRandomField}","page":"Home","title":"VLBIImagePriors.scalematrix","text":"scalematrix(m::MarkovRandomField)\n\nReturn the scale matrix for the Markov Random field. For a Gaussian Markov random field this corresponds to the precision matrix of the Gaussian field.\n\nFor other random processes this is the metric of the inner product, i.e. Q in\n\nxᵀQx\n\nwhich is the distance from the origin to x using the metric Q.\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.to_real!-Tuple{AdditiveLR, Any, Any}","page":"Home","title":"VLBIImagePriors.to_real!","text":"to_real(t::LogRatioTransform, y)\n\nTransform the value u that lives on the simplex to a value in the real vector space. See subtypes(LogRatioTransform) for a list of possible transformations.\n\nThe inverse of this transform is given by to_simplex(t, x).\n\nExample\n\njulia> y = randn(100)\njulia> y .= y./sum(y)\njulia> x = similar(y)\njulia> to_real!(CenteredLR(), x, y)\njulia> xar = similar(y, 99)\njulia> to_real!(AdditiveLR(), xar, y)\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.to_real-Tuple{VLBIImagePriors.LogRatioTransform, Any}","page":"Home","title":"VLBIImagePriors.to_real","text":"to_real(t::LogRatioTransform, u)\n\nTransform the value u that lives on the simplex to a value x in the real vector space. See subtypes(LogRatioTransform) for a list of possible transformations.\n\nThe inverse of this transform is given by to_simplex!(t, y, x).\n\nExample\n\njulia> y = randn(100)\njulia> y .= y./sum(y)\njulia> to_real(CenteredLR(), y)\njulia> to_real(AdditiveLR(), y)\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.to_simplex!-Tuple{AdditiveLR, Any, Any}","page":"Home","title":"VLBIImagePriors.to_simplex!","text":"to_simplex!(t::LogRatioTransform, y, x)\n\nTransform the vector x assumed to be a real valued array to the simplex using the log-ratio transform t and stores the value in y.\n\nThe inverse of this transform is given by to_real!(t, x, y) where y is a vector that sums to unity, i.e. it lives on the simplex.\n\nExample\n\njulia> x = randn(100)\njulia> to_simplex(CenteredLR(), x)\njulia> to_simplex(AdditiveLR(), x)\n\n\n\n\n\n\n\n","category":"method"},{"location":"#VLBIImagePriors.to_simplex-Tuple{VLBIImagePriors.LogRatioTransform, Any}","page":"Home","title":"VLBIImagePriors.to_simplex","text":"to_simplex(t::LogRatioTransform, x)\n\nTransform the vector x assumed to be a real valued array to the simplex using the log-ratio transform t. See subtypes(LogRatioTransform) for a list of possible transformations.\n\nThe inverse of this transform is given by to_real(t, y) where y is a vector that sums to unity, i.e. it lives on the simplex.\n\nExample\n\njulia> x = randn(100)\njulia> to_simplex(CenteredLR(), x)\njulia> to_simplex(AdditiveLR(), x)\n\n\n\n\n\n\n\n","category":"method"}]
}
